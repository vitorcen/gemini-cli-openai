import { Hono } from "hono";
import { Env, ChatCompletionRequest, ChatCompletionResponse, Tool } from "../types";
import { geminiCliModels, DEFAULT_MODEL, getAllModelIds } from "../models";
import { OPENAI_MODEL_OWNER } from "../config";
import { DEFAULT_THINKING_BUDGET } from "../constants";
import { AuthManager } from "../auth";
import { GeminiApiClient } from "../gemini-client";
import { createOpenAIStreamTransformer } from "../stream-transformer";

/**
 * OpenAI-compatible API routes for models and chat completions.
 */
export const OpenAIRoute = new Hono<{ Bindings: Env }>();

// Simple repetition guard: detect if the last assistant message text
// appeared at least (threshold) times in the recent history.
function detectRepetition(
    messages: ChatCompletionRequest["messages"],
    threshold: number = 3
): { repeated: boolean; sample?: string } {
    if (!Array.isArray(messages) || messages.length === 0 || threshold <= 1) return { repeated: false };
    // find last assistant message
    let last: string | undefined;
    for (let i = messages.length - 1; i >= 0; i--) {
        const m = messages[i];
        if (m.role === "assistant") {
            const txt = typeof m.content === "string"
                ? m.content
                : Array.isArray(m.content)
                ? m.content.map((p: any) => p?.text ?? "").join(" ")
                : "";
            last = txt.trim();
            break;
        }
    }
    if (!last) return { repeated: false };
    let count = 0;
    const start = Math.max(0, messages.length - 40);
    for (let j = start; j < messages.length; j++) {
        const mm = messages[j];
        if (mm.role !== "assistant") continue;
        const txt = typeof mm.content === "string"
            ? mm.content
            : Array.isArray(mm.content)
            ? mm.content.map((p: any) => p?.text ?? "").join(" ")
            : "";
        if (txt.trim() === last) count++;
    }
    return { repeated: count >= threshold, sample: last.slice(0, 120) };
}

// Translate client tools into custom tools plus a native search enable flag
function translateTools(tools: Tool[] | undefined): { customTools: Tool[] | undefined; enableSearch: boolean } {
    if (!tools || !Array.isArray(tools) || tools.length === 0) {
        return { customTools: undefined, enableSearch: false };
    }
    let enableSearch = false;
    const customTools = tools.filter((t) => {
        const name = t.function?.name;
        if (name === "web_search" || name === "brave_web_search") {
            enableSearch = true;
            return false; // remove unsupported tool name for Gemini
        }
        return true;
    });
    return { customTools: customTools.length > 0 ? customTools : undefined, enableSearch };
}

// List available models
OpenAIRoute.get("/models", async (c) => {
	const modelData = getAllModelIds().map((modelId) => ({
		id: modelId,
		object: "model",
		created: Math.floor(Date.now() / 1000),
		owned_by: OPENAI_MODEL_OWNER
	}));

	return c.json({
		object: "list",
		data: modelData
	});
});

// Chat completions endpoint
OpenAIRoute.post("/chat/completions", async (c) => {
	try {
		console.log("Chat completions request received");
		const body = await c.req.json<ChatCompletionRequest>();
		const model = body.model || DEFAULT_MODEL;
		const messages = body.messages || [];
		// OpenAI API compatibility: stream defaults to true unless explicitly set to false
		const stream = body.stream !== false;

		// Check environment settings for real thinking
		const isRealThinkingEnabled = c.env.ENABLE_REAL_THINKING === "true";
		let includeReasoning = isRealThinkingEnabled; // Automatically enable reasoning when real thinking is enabled
		let thinkingBudget = body.thinking_budget ?? DEFAULT_THINKING_BUDGET; // Default to dynamic allocation

		// Newly added parameters
		const generationOptions = {
			max_tokens: body.max_tokens,
			temperature: body.temperature,
			top_p: body.top_p,
			stop: body.stop,
			presence_penalty: body.presence_penalty,
			frequency_penalty: body.frequency_penalty,
			seed: body.seed,
			response_format: body.response_format
		};

		// Handle effort level mapping to thinking_budget (check multiple locations for client compatibility)
		const reasoning_effort =
			body.reasoning_effort || body.extra_body?.reasoning_effort || body.model_params?.reasoning_effort;
		if (reasoning_effort) {
			includeReasoning = true; // Effort implies reasoning
			const isFlashModel = model.includes("flash");
			switch (reasoning_effort) {
				case "low":
					thinkingBudget = 1024;
					break;
				case "medium":
					thinkingBudget = isFlashModel ? 12288 : 16384;
					break;
				case "high":
					thinkingBudget = isFlashModel ? 24576 : 32768;
					break;
				case "none":
					thinkingBudget = 0;
					includeReasoning = false;
					break;
			}
		}

		// Repetition guard: block repeated assistant output loops
		const enableRepeatGuard = c.env.ENABLE_REPEAT_GUARD !== "false"; // default on
		const repeatThreshold = Number(c.env.REPEAT_GUARD_THRESHOLD || 3);
		if (enableRepeatGuard) {
			const rep = detectRepetition(messages, isNaN(repeatThreshold) ? 3 : repeatThreshold);
			if (rep.repeated) {
				const display = `RepeatGuard: detected identical assistant output repeated >= ${repeatThreshold} times.\nPlease adjust your prompt or stop the loop.\n\nSample: ${rep.sample}`;
				console.warn(display);
				// Surface a normal assistant response so CCR UI shows it and stops retrying
				if (stream) {
					const { readable, writable } = new TransformStream();
					const writer = writable.getWriter();
					const openAITransformer = createOpenAIStreamTransformer(model);
					const openAIStream = readable.pipeThrough(openAITransformer);
					(async () => {
						try {
							await writer.write({ type: "text", data: `❌ ${display}` });
						} finally {
							await writer.close();
						}
					})();
					return new Response(openAIStream, {
						headers: {
							"Content-Type": "text/event-stream",
							"Cache-Control": "no-cache",
							Connection: "keep-alive"
						}
					});
				} else {
					const response: ChatCompletionResponse = {
						id: `chatcmpl-${crypto.randomUUID()}`,
						object: "chat.completion",
						created: Math.floor(Date.now() / 1000),
						model: model,
						choices: [
							{
								index: 0,
								message: { role: "assistant", content: `❌ ${display}` },
								finish_reason: "stop"
							}
						]
					};
					return new Response(JSON.stringify(response), {
						status: 200,
						headers: {
							"Content-Type": "application/json",
							"X-CCR-Severity": "error"
						}
					});
				}
			}
		}

		const tool_choice = body.tool_choice;

		// Translate web_search/brave_web_search -> native search flag + filtered custom tools
		const translation = translateTools(body.tools as any);
		const enableGoogleSearch = translation.enableSearch;
		let tools = translation.customTools as any;
		if (enableGoogleSearch) {
			console.log("[ToolTranslation] web_search detected -> enable native Google Search");
		}

		console.log(
			`[Parse] model=${model} messageCount=${messages.length} stream=${stream} includeReasoning=${includeReasoning} thinkingBudget=${thinkingBudget} toolsCount=${tools?.length || 0} tool_choice=${tool_choice || "auto"} web_search=${enableGoogleSearch ? "on" : "off"}`
		);

		if (!messages.length) {
			return c.json({ error: "messages is a required field" }, 400);
		}

		// Validate model
		if (!(model in geminiCliModels)) {
			return c.json(
				{
					error: `Model '${model}' not found. Available models: ${getAllModelIds().join(", ")}`
				},
				400
			);
		}

		// Check if the request contains images and validate model support
		const hasImages = messages.some((msg) => {
			if (Array.isArray(msg.content)) {
				return msg.content.some((content) => content.type === "image_url");
			}
			return false;
		});

		if (hasImages && !geminiCliModels[model].supportsImages) {
			return c.json(
				{
					error: `Model '${model}' does not support image inputs. Please use a vision-capable model like gemini-2.5-pro or gemini-2.5-flash.`
				},
				400
			);
		}

		// Extract system prompt and user/assistant messages
		let systemPrompt = "";
		const otherMessages = messages.filter((msg) => {
			if (msg.role === "system") {
				// Handle system messages with both string and array content
				if (typeof msg.content === "string") {
					systemPrompt = msg.content;
				} else if (Array.isArray(msg.content)) {
					// For system messages, only extract text content
					const textContent = msg.content
						.filter((part) => part.type === "text")
						.map((part) => part.text || "")
						.join(" ");
					systemPrompt = textContent;
				}
				return false;
			}
			return true;
		});

		// Initialize services
		const authManager = new AuthManager(c.env);
		const geminiClient = new GeminiApiClient(c.env, authManager);

		// Test authentication first
		try {
			await authManager.initializeAuth();
		} catch (authError: unknown) {
			const errorMessage = authError instanceof Error ? authError.message : String(authError);
			console.error("Authentication failed:", errorMessage);
			return c.json({ error: "Authentication failed: " + errorMessage }, 401);
		}

		if (stream) {
			// Streaming response
			const { readable, writable } = new TransformStream();
			const writer = writable.getWriter();
			const openAITransformer = createOpenAIStreamTransformer(model);
			const openAIStream = readable.pipeThrough(openAITransformer);

			// Asynchronously pipe data from Gemini to transformer
			(async () => {
				try {
					const geminiStream = geminiClient.streamContent(model, systemPrompt, otherMessages, {
						includeReasoning,
						thinkingBudget,
						tools,
					tool_choice,
						enable_search: enableGoogleSearch,
						...generationOptions
					});

					for await (const chunk of geminiStream) {
						await writer.write(chunk);
					}
					// Stream completed successfully (no log needed, token usage already printed)
					await writer.close();
				} catch (streamError: unknown) {
					const errorMessage = streamError instanceof Error ? streamError.message : String(streamError);
					console.error("Stream error:", errorMessage);
					// Try to write an error chunk before closing
					await writer.write({
						type: "text",
						data: `Error: ${errorMessage}`
					});
					await writer.close();
				}
			})();

			// Return streaming response
			return new Response(openAIStream, {
				headers: {
					"Content-Type": "text/event-stream",
					"Cache-Control": "no-cache",
					Connection: "keep-alive",
					"Access-Control-Allow-Origin": "*",
					"Access-Control-Allow-Methods": "GET, POST, OPTIONS",
					"Access-Control-Allow-Headers": "Content-Type, Authorization"
				}
			});
		} else {
			// Non-streaming response
			try {
				console.log("Starting non-streaming completion");
				const completion = await geminiClient.getCompletion(model, systemPrompt, otherMessages, {
					includeReasoning,
					thinkingBudget,
					tools,
					tool_choice,
					enable_search: enableGoogleSearch,
						...generationOptions
				});

				const response: ChatCompletionResponse = {
					id: `chatcmpl-${crypto.randomUUID()}`,
					object: "chat.completion",
					created: Math.floor(Date.now() / 1000),
					model: model,
					choices: [
						{
							index: 0,
							message: {
								role: "assistant",
								content: completion.content,
								tool_calls: completion.tool_calls
							},
							finish_reason: completion.tool_calls && completion.tool_calls.length > 0 ? "tool_calls" : "stop"
						}
					]
				};

				// Add usage information if available
				if (completion.usage) {
					response.usage = {
						prompt_tokens: completion.usage.inputTokens || 0,
						completion_tokens: completion.usage.outputTokens || 0,
						total_tokens: (completion.usage.inputTokens || 0) + (completion.usage.outputTokens || 0) + (completion.usage.thinkingTokens || 0)
					};
				}

				console.log("Non-streaming completion successful");
				return c.json(response);
			} catch (completionError: unknown) {
				const errorMessage = completionError instanceof Error ? completionError.message : String(completionError);
				console.error("Completion error:", errorMessage);
				return c.json({ error: errorMessage }, 500);
			}
		}
	} catch (e: unknown) {
		const errorMessage = e instanceof Error ? e.message : String(e);
		console.error("Top-level error:", e);
		return c.json({ error: errorMessage }, 500);
	}
});
